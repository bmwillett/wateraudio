{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading files...\n",
      "loaded:\n",
      "for label=cold, 22 training clips and 7 test clips with total length 869 seconds\n",
      "for label=hot, 22 training clips and 7 test clips with total length 871 seconds\n"
     ]
    }
   ],
   "source": [
    "%run 'init.py'\n",
    "all_clips=load_files()\n",
    "\n",
    "maxsd=5\n",
    "maxsd_filter = lambda sample : np.max(sample)/np.std(sample)<maxsd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****************\n",
    "<font size=\"6\">Logistic Regression</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****************\n",
    "<font size=\"4\">using raw wav data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test results:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5178571428571429"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_duration=0.5\n",
    "sr=default_sr\n",
    "sample_size=int(sr*sample_duration)\n",
    "sample_size_log_raw=sample_size\n",
    "\n",
    "train_filter=test_filter=no_filter\n",
    "\n",
    "train_clips=[]\n",
    "for label in labels:\n",
    "    train_clips+=[clip for clip in all_clips[label] if (clip.sr==sr and clip.kind=='train' and clip.size>=sample_size)]\n",
    "\n",
    "n_samples=sum([int(clip.size/sample_size) for clip in train_clips])\n",
    "X=np.zeros((n_samples,sample_size))\n",
    "y=np.zeros(n_samples)\n",
    "params_log_raw=None\n",
    "\n",
    "def getdata_log_raw(sample,params):\n",
    "    return sample\n",
    "\n",
    "for i,(sample,_,label) in enumerate(sample_generator(train_clips,sample_duration,sample_filter=train_filter)):\n",
    "    X[i,:]=getdata_log_raw(sample,params_log_raw)\n",
    "    y[i]=label2ind[label]\n",
    "\n",
    "clf_log_raw = LogisticRegression(random_state=0).fit(X, y)\n",
    "\n",
    "clf_log_raw.fit(X, y)\n",
    "\n",
    "test_clips=[]\n",
    "for label in labels:\n",
    "    test_clips+=[clip for clip in all_clips[label] if (clip.sr==sr and clip.kind=='test' and clip.size>=sample_size)]\n",
    "    \n",
    "n_samples=sum([int(clip.size/sample_size) for clip in test_clips])\n",
    "X_test=np.zeros((n_samples,sample_size))\n",
    "y_test=np.zeros(n_samples)\n",
    "\n",
    "for i,(sample,_,label) in enumerate(sample_generator(test_clips,sample_duration,sample_filter=test_filter)):\n",
    "    X_test[i,:]=getdata_log_raw(sample,params_log_raw)\n",
    "    y_test[i]=label2ind[label]\n",
    "\n",
    "print(\"test results:\")\n",
    "clf_log_raw.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****************\n",
    "<font size=\"4\">using spectrogram data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:938: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:938: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test results:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7952380952380952"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_duration=1\n",
    "sr=default_sr\n",
    "sample_size=int(sr*sample_duration)\n",
    "sample_size_log_spec=sample_size\n",
    "\n",
    "n_fft=256\n",
    "hop_length=128\n",
    "n_freqs=n_fft//2+1\n",
    "\n",
    "train_filter=test_filter=maxsd_filter\n",
    "\n",
    "train_clips=[]\n",
    "for label in labels:\n",
    "    train_clips+=[clip for clip in all_clips[label] if (clip.sr==sr and clip.kind=='train' and clip.size>=sample_size)]\n",
    "\n",
    "n_samples=sum([int(clip.size/sample_size) for clip in train_clips])\n",
    "X=np.zeros((n_samples,n_freqs))\n",
    "y=np.zeros(n_samples)\n",
    "params_log_spec=n_fft,hop_length\n",
    "\n",
    "def getdata_log_spec(sample,params):\n",
    "    n_fft,hop_length=params\n",
    "    return np.sum(np.abs(librosa.stft(sample, n_fft=n_fft,hop_length=hop_length)),axis=1)\n",
    "\n",
    "for i,(sample,_,label) in enumerate(sample_generator(train_clips,sample_duration,sample_filter=train_filter)):\n",
    "    X[i,:]=getdata_log_spec(sample,params_log_spec)\n",
    "    y[i]=label2ind[label]\n",
    "\n",
    "clf_log_spec = LogisticRegression(C=2,random_state=0).fit(X, y)\n",
    "\n",
    "clf_log_spec.fit(X, y)\n",
    "\n",
    "test_clips=[]\n",
    "for label in labels:\n",
    "    test_clips+=[clip for clip in all_clips[label] if (clip.sr==sr and clip.kind=='test' and clip.size>=sample_size)]\n",
    "    \n",
    "n_samples=sum([int(clip.size/sample_size) for clip in test_clips])\n",
    "X_test=np.zeros((n_samples,n_freqs))\n",
    "y_test=np.zeros(n_samples)\n",
    "\n",
    "for i,(sample,_,label) in enumerate(sample_generator(test_clips,sample_duration,sample_filter=test_filter)):\n",
    "    X_test[i,:]=getdata_log_spec(sample,params_log_spec)\n",
    "    y_test[i]=label2ind[label]\n",
    "\n",
    "print(\"test results:\")\n",
    "clf_log_spec.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hot', 'hot', 'cold', 'hot', 'hot', 'hot', 'hot', 'hot', 'hot', 'cold']\n"
     ]
    }
   ],
   "source": [
    "log_spec_predictor=predictor(clf_log_spec,getdata_log_spec,params_log_spec,sample_size_log_spec)\n",
    "\n",
    "# pick 10 random 'cold' samples to test\n",
    "to_test=[sample for sample,_,_ in sample_generator(all_clips['hot'], sample_size_log_spec/default_sr, max_iters=10,random=True)]\n",
    "\n",
    "print(log_spec_predictor.test_samples(to_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****************\n",
    "<font size=\"6\">SVM</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****************\n",
    "<font size=\"4\">using spectrogram data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test results:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7690476190476191"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_duration=1\n",
    "sr=default_sr\n",
    "sample_size=int(sr*sample_duration)\n",
    "sample_size_svm=sample_size\n",
    "\n",
    "n_fft=256\n",
    "hop_length=128\n",
    "n_freqs=n_fft//2+1\n",
    "\n",
    "train_filter=test_filter=no_filter\n",
    "\n",
    "train_clips=[]\n",
    "for label in labels:\n",
    "    train_clips+=[clip for clip in all_clips[label] if (clip.sr==sr and clip.kind=='train' and clip.size>=sample_size)]\n",
    "\n",
    "n_samples=sum([int(clip.size/sample_size) for clip in train_clips])\n",
    "X=np.zeros((n_samples,n_freqs))\n",
    "y=np.zeros(n_samples)\n",
    "params_svm=n_fft,hop_length\n",
    "\n",
    "def getdata_svm(sample,params):\n",
    "    n_fft,hop_length=params\n",
    "    return np.sum(np.abs(librosa.stft(sample, n_fft=n_fft,hop_length=hop_length)),axis=1)\n",
    "\n",
    "for i,(sample,_,label) in enumerate(sample_generator(train_clips,sample_duration,sample_filter=train_filter)):\n",
    "    X[i,:]=getdata_svm(sample,params_svm)\n",
    "    y[i]=label2ind[label]\n",
    "\n",
    "clf_svm = svm.SVC(C=0.00001,kernel='poly',degree=2,gamma=0.01,tol=1e-4)\n",
    "\n",
    "clf_svm.fit(X, y)\n",
    "\n",
    "test_clips=[]\n",
    "for label in labels:\n",
    "    test_clips+=[clip for clip in all_clips[label] if (clip.sr==sr and clip.kind=='test' and clip.size>=sample_size)]\n",
    "    \n",
    "n_samples=sum([int(clip.size/sample_size) for clip in test_clips])\n",
    "X_test=np.zeros((n_samples,n_freqs))\n",
    "y_test=np.zeros(n_samples)\n",
    "\n",
    "for i,(sample,_,label) in enumerate(sample_generator(test_clips,sample_duration,sample_filter=test_filter)):\n",
    "    X_test[i,:]=getdata_svm(sample,params_svm)\n",
    "    y_test[i]=label2ind[label]\n",
    "\n",
    "print(\"test results:\")\n",
    "clf_svm.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "water is currently: <font color='red'>HOT</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "svm_predictor=predictor(clf_svm,getdata_svm,params_svm,sample_size_svm)\n",
    "svm_predictor.continuous_record_and_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****************\n",
    "<font size=\"6\">CNN on spectrogram</font>\n",
    "\n",
    "-> found some references suggesting, for audio classification, use CNN on mel spectrogram or mel-frequency cepstral coefficients\n",
    "    - https://web.archive.org/web/20110717210107/http://www.wcl.ece.upatras.gr/ganchev/Papers/ganchev17.pdf\n",
    "    - https://towardsdatascience.com/audio-classification-using-fastai-and-on-the-fly-frequency-transforms-4dbe1b540f89\n",
    "    - https://en.wikipedia.org/wiki/Mel-frequency_cepstrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnAAAAJNCAYAAACx90jQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de7Ck6V0f9u9z+lznzHVXq5XYXWkXs5gIgQtYCxnKFIVcQkAKyUSoRFJmISqrXJGN7SROIHGVHC5V4LgsgwMkKrSOwBihKJRRYYxKJQkIqQhYkNCNCC0CoV1Wl52ZnfvMufSTP6Y3DMvszPk1c86ZZ/bzqTo13W8/v/4979tvv/2dt0+fbr33AAAwjoX9ngAAADUCHADAYAQ4AIDBCHAAAIMR4AAABiPAAQAMZnG/J7DXbj+41l9425Edj9+LP7PSWtv1Hlmc1Gu2p+WSPt39mj7HvBbmWP82KdYszPE4TufYv4r75DyPyTzq26v+/8c2R03VXNtre3v3+8xxnNiTY8s8PeY4rs51LK7W7NE2rh7D5jquzrG92qT2/FqoPuf3SnE9ksz1ejfPMWweH/jU40/03u94+vJnXYB74W1H8mv/5O/seHzf3NzF2Vy2sLK86z0mR46Wa6bnzpVrts6crdecv1Aav3nmfLnHym2HyzVLx3Ye9JNkYXW13GN68WK5pl/aqPXYqI2f1+Lh2jZeOLBW7tFW6tu4anqhvn9tnz5T71N97Fv9xWJhafcP8W2O41ff3KrXzBGSp8XnSvk/IZlvG2+dre1jm8VjZJL0rfr2Wjp0oDj+YLnHPFrxP+AL6+vlHtunTpdrJgeLfeb8D9X6677/U1db7i1UAIDBCHAAAIMR4AAABiPAAQAMRoADABiMAAcAMBgBDgBgMAIcAMBgBDgAgMEIcAAAgxHgAAAGI8ABAAxGgAMAGMzifk9gz00WMjl0cMfDF9bWave/tl6cUJLpdr2m6tDRcslkOi3XLHz+8XLN0sZGafyBxaVyj3b4SLkm1T6nTpZbLKyslmtSrZlM6j3mqVmuzasvLZdbTIs9kmR7pfacnJx7stxj6Uz9sc+5M7XxC/XHpN/+3HJNu3Cu2KSXe0yPPKdc07Zqx4nLNZu1gu2tco95LBX7rG3V5zV98kS5phePxZMX3FfuMY95jhVVSxcv1Is2a9tr+sTn6j2uwRk4AIDBCHAAAIMR4AAABiPAAQAMRoADABiMAAcAMBgBDgBgMAIcAMBgBDgAgMEIcAAAgxHgAAAGI8ABAAxGgAMAGIwABwAwGAEOAGAwAhwAwGAEOACAwQhwAACDEeAAAAYjwAEADEaAAwAYjAAHADAYAQ4AYDACHADAYAQ4AIDBCHAAAIMR4AAABiPAAQAMZnG/J7DXNk+fy+Pv/a0dj1+7/XDp/ttCq04pfdrLNVUrR9bLNZO11XLN5plz5Zrp5lZp/MJSfbfdOHO+XNMWav+/WTywUu6xMJmUazbPXSiNX1yrz2txfa1cU91eS7cdLffYOnWmXNMWa9t4c3Oz3GP70ka5pm9tl8ZX1yNJFpb+qFxTfQ5XH/ckWX3eHeWavl3bXkmycfJUbfyps+Ue81g5dqg0frpRO0YmybnPHC/XVB/LQ0+cKPfYvnCpXFN9XV1YWir3mMxxnNw8W3tdmec4cS3OwAEADEaAAwAYzK4FuNbaQ621z7XWPnLFsttaa+9urX1i9u+x2fLWWvux1tojrbUPtda+8oqaB2fjP9Fae/CK5V/VWvvwrObHWmv19y4BAAa0m2fg/vckr3jasu9N8p7e+/1J3jO7niTflOT+2c/rk/xkcjnwJXljkq9O8pIkb3wq9M3G/N0r6p7eCwDglrRrAa73/utJnv4bjq9M8tbZ5bcmedUVy3+6X/b+JEdba89P8o1J3t17P9F7P5nk3UleMbvtcO/9/b33nuSnr7gvAIBb2l7/DtydvffHZ5c/k+TO2eW7knz6inGPzpZda/mjV1kOAHDL27cPMczOnO3+389I0lp7fWvt4dbawydu8Md4AQD22l4HuM/O3v7M7N/PzZY/luSeK8bdPVt2reV3X2X5VfXe39x7f6D3/sBtK8t/6ZUAANhPex3g3pnkqU+SPpjkF69Y/p2zT6O+NMmp2Vut70ry8tbasdmHF16e5F2z20631l46+/Tpd15xXwAAt7Rd+yaG1trPJfn6JM9prT2ay58m/eEkb2+tvS7Jp5K8Zjb8l5N8c5JHkpxP8t1J0ns/0Vr7gSS/PRv3/b33pz4Y8V/l8idd15L8x9kPAMAtb9cCXO/9O57hppddZWxP8oZnuJ+Hkjx0leUPJ3nxX2aOAAAj8k0MAACDEeAAAAYjwAEADEaAAwAYjAAHADAYAQ4AYDACHADAYAQ4AIDBCHAAAIMR4AAABiPAAQAMRoADABiMAAcAMBgBDgBgMAIcAMBgBDgAgMEs7vcE9trSwQO582u+fMfjW9v9jLuwvl6uaUu1h65vbpV7TM+dK9cs33F7uaatrtbGrx8q98j5+rpkebk2fqW2HkmSrc16zfZ2bfzBw/UeC5N6zbnTtfGXLpZbtIX683HhQO351ed4TFqxx1zm2e8n9UP86qULtYLq/pjM91zZrh/DljY3SuPXThwv9+hzrH+b1J5fbXml3GPtnueXa6qvEwsrxWNkkizW98mF6voXt2+STM+eKddMDh0sjd8+c7bc41qcgQMAGIwABwAwGAEOAGAwAhwAwGAEOACAwQhwAACDEeAAAAYjwAEADEaAAwAYjAAHADAYAQ4AYDACHADAYAQ4AIDBCHAAAIMR4AAABiPAAQAMRoADABiMAAcAMBgBDgBgMAIcAMBgBDgAgMEIcAAAgxHgAAAGI8ABAAxGgAMAGIwABwAwGAEOAGAwAhwAwGAW93sCe60tLWXx+XfteHw//WTp/rfPnK1OKX1rs1zTVlZqPS7V55WFVi5pS0v1PtNeGz/H9spifVffPnG8NH7jiRPlHgtzbK/FI4dK4yflDknW1ssl/czp0vjN4/Xt1Vp9n7z4+GdrPSb1LbZ05HC5Jn1aGr6wXtu+SdKWl8s1fWOjNH5hZbXcIxuXyiXT4v6VJOm1Y8v0Um3dk6Rv1o9HGydPlcZP1urbeLJae41Iku2LxcflTLnFXPPqq7X1nxw9Uu4xLe73SdKL+8vGk3NssGtwBg4AYDACHADAYAQ4AIDBCHAAAIMR4AAABiPAAQAMRoADABiMAAcAMBgBDgBgMAIcAMBgBDgAgMEIcAAAgxHgAAAGI8ABAAxGgAMAGIwABwAwGAEOAGAwAhwAwGAEOACAwQhwAACDEeAAAAYjwAEADEaAAwAYjAAHADAYAQ4AYDACHADAYAQ4AIDBLO73BPba9sp6zv6Vr9rx+Mn2Run+FzfOVaeU7XJFcml5vTR+6eLpco+tYo8kSav/n2Dp/Ild79HnqFl43qXS+LULZ8s9tg8dK9dU12Wr3CHpi8v1mufdVxq/Pan3WNy8UK5ZvXCmVjCd4xm5uFSvKdpeXivX9El9Xm17szR+c47HcZ7n4zzH1oWNi6Xxk63a8T5J2ma9ZmmzdmxJ7+Ue8+yTiyu1fawvr5Z7ZDotl7TN2uO4efC2co+Fe4qPSZK+VFv/1e15jsZJfuChqy52Bg4AYDACHADAYAQ4AIDBCHAAAIMR4AAABiPAAQAMRoADABiMAAcAMBgBDgBgMAIcAMBgBDgAgMEIcAAAgxHgAAAGI8ABAAxGgAMAGMy+BLjW2j9urX20tfaR1trPtdZWW2v3tdZ+s7X2SGvt51try7OxK7Prj8xuv/eK+/m+2fKPt9a+cT/WBQBgr+15gGut3ZXke5I80Ht/cZJJktcm+ZEkb+q9f1GSk0leNyt5XZKTs+Vvmo1La+1Fs7ovTfKKJD/RWpvs5boAAOyH/XoLdTHJWmttMcmBJI8n+YYk75jd/tYkr5pdfuXsema3v6y11mbL39Z7v9R7/6MkjyR5yR7NHwBg3+x5gOu9P5bkXyT5k1wObqeS/E6SJ3vvW7Nhjya5a3b5riSfntVuzcbffuXyq9QAANyy9uMt1GO5fPbsviRfkGQ9l98C3c2er2+tPdxae/j4yZO72QoAYNftx1uofyvJH/XeP99730zyC0m+NsnR2VuqSXJ3ksdmlx9Lck+SzG4/kuT4lcuvUvPn9N7f3Ht/oPf+wO3Hjt3o9QEA2FP7EeD+JMlLW2sHZr/L9rIkH0vyviSvno15MMkvzi6/c3Y9s9vf23vvs+WvnX1K9b4k9yf5rT1aBwCAfbN4/SE3Vu/9N1tr70jyu0m2knwgyZuT/Ickb2ut/eBs2VtmJW9J8jOttUeSnMjlT56m9/7R1trbczn8bSV5Q+99e09XBgBgH+x5gEuS3vsbk7zxaYs/mat8irT3fjHJtz/D/fxQkh+64RMEALiJ+SYGAIDBCHAAAIMR4AAABiPAAQAMZl8+xLCfFs6czMr7fmHH4/vmZun+28pydUrJpP4Vrsutlr23Tp8u91g8sFauacv19d8+c7ZWsFD/f8fCUn1X39rcuv6gK3vMse5tjnktFPeX6cVL5R59Y6Nck+m0NHx5fb3cYp7tNb1woVxTtXm6uA8nWVisPY5b5+rrsTDH8Wj5ttrfymy99rgnyfbZ8+WarK2USzYv1Pb9eR7HzTkel+2N2rFl/Xm3l3ssHT1UrpkWj3mLhw7We1yqH1umFy+Wxs+z32eO4/ekeiy+wcciZ+AAAAYjwAEADEaAAwAYjAAHADAYAQ4AYDACHADAYAQ4AIDBCHAAAIMR4AAABiPAAQAMRoADABiMAAcAMBgBDgBgMAIcAMBgBDgAgMEIcAAAgxHgAAAGI8ABAAxGgAMAGIwABwAwGAEOAGAwAhwAwGAEOACAwQhwAACDEeAAAAYjwAEADEaAAwAYjAAHADCYxf2ewF5ra2tZetGX7dr9TxeXyzVtul3vs7peGr/Yp/UeywfKNX2OPktnjpfGt97LPfpkqVwzKY6frqyVe/SFapdke2m1NH7x7Mlyj8WL58o1Ke7HfXml3GLzyHPLNZONC7WCOfbh1UvFHqnvk0uT+r7SNjfKNdX1b1ub5RZt/XC5Zlrc75Nkcau2/iuffqTcY3rhfLlm4fY7aj0O317u0RfrL++L1WPrtP5cWbxwtlyT4r4/z76S1uolxf1rcY7nyrU4AwcAMBgBDgBgMAIcAMBgBDgAgMEIcAAAgxHgAAAGI8ABAAxGgAMAGIwABwAwGAEOAGAwAhwAwGAEOACAwQhwAACDEeAAAAYjwAEADEaAAwAYjAAHADAYAQ4AYDACHADAYAQ4AIDBCHAAAIMR4AAABiPAAQAMRoADABiMAAcAMBgBDgBgMAIcAMBgFvd7AntuOk0unt/5+HNnS3ffT54sTiiZbm+XaxZWlkvj29paucdkZbVck63Ncsn04sXS+L61Ve6xcGC9XNOLj0vbrs+rnyvsizOLqyul8W21/jj2ckXSNzZK46fnL5R7LB19tFyT4uM4z7y253gOp09LwydHjpZbtJXavpIkWaj9v76fP1du0ebYXouLS/U+xePe5mc/W+7RN+vHvEnxudKe+Fy9xzzH74VWGr41x+vd9qXauifJ4m3HSuPbZFLusX36dLmmqs+x7tfiDBwAwGAEOACAwQhwAACDEeAAAAYjwAEADEaAAwAYjAAHADAYAQ4AYDACHADAYAQ4AIDBCHAAAIMR4AAABiPAAQAMRoADABiMAAcAMBgBDgBgMAIcAMBgBDgAgMEIcAAAgxHgAAAGI8ABAAxGgAMAGIwABwAwGAEOAGAwAhwAwGAEOACAwQhwAACDEeAAAAazuB9NW2tHk/xUkhcn6Un+yyQfT/LzSe5N8sdJXtN7P9laa0l+NMk3Jzmf5Lt67787u58Hk/zT2d3+YO/9rdfr3Rcm6QcO7XyylbFJ2vNeUBqfJNO1g+WajaUDpfEL081yj7a1Ua7pi8vlmunCUq1Ha/Uek1qPJJlsXqyN365vr6UTj5drtg8eqY1fXCn3yMKkXFJ9XBbPnar3KFfULUy360XTrXJJX6k9h7dW1ss90qflks3Vw6Xxy+dPlnssnj5ertler80rSc4drR2PD9xdfz4uPfFYuSaT4kvvHM/HzaPPLde04v6yeOyJco9s1V+Lto8V12WO/b61+vmstlF7jVgojr/u/d3Qe9u5H03yK733L0ny15L8fpLvTfKe3vv9Sd4zu54k35Tk/tnP65P8ZJK01m5L8sYkX53kJUne2Fo7tpcrAQCwH/Y8wLXWjiT5uiRvSZLe+0bv/ckkr0zy1Bm0tyZ51ezyK5P8dL/s/UmOttaen+Qbk7y7936i934yybuTvGIPVwUAYF/sxxm4+5J8Psm/aa19oLX2U6219SR39t6fOn/9mSR3zi7fleTTV9Q/Olv2TMsBAG5p+xHgFpN8ZZKf7L1/RZJz+bO3S5MkvfeeG/jrLq2117fWHm6tPfzEqdM36m4BAPbFfgS4R5M82nv/zdn1d+RyoPvs7K3RzP793Oz2x5Lcc0X93bNlz7T8L+i9v7n3/kDv/YHnHKn/IiwAwM1kzwNc7/0zST7dWvurs0UvS/KxJO9M8uBs2YNJfnF2+Z1JvrNd9tIkp2Zvtb4ryctba8dmH154+WwZAMAtbV/+jEiSf5DkZ1try0k+meS7czlMvr219rokn0rymtnYX87lPyHySC7/GZHvTpLe+4nW2g8k+e3ZuO/vvZ/Yu1UAANgf+xLgeu8fTPLAVW562VXG9iRveIb7eSjJQzd2dgAANzffxAAAMBgBDgBgMAIcAMBgBDgAgMEIcAAAgxHgAAAGI8ABAAxGgAMAGIwABwAwGAEOAGAwAhwAwGAEOACAwQhwAACDEeAAAAazowDXWvubrbXJ05Z95e5MCQCAa9npGbh3JXlva+25Vyz7qV2YDwAA17HTAPfxJP9zkl9rrX3NbFnbnSkBAHAtizsc13vvv9Ra+3iSn2+tPZSk7+K8AAB4Bjs9A9eSpPf+iSRfN/v58t2aFAAAz2xHZ+B6719xxeWzSV7TWnvBrs1qF20+cTx/+m/+3a7df1uov7O8sDi5/qCnOfSCO0vjt8sdkksnz5Rr+nRarlm9/Uhp/PKRQ+UeFz7z+XLNxfOXSuMXV5fLPbY3tso11X1snv1rHpOV2vpvzvFcmUcrrv/5z57cpZn8edX9ZbKyVO4x3Zxj/ypur82t+tFl89zFcs3K0YPlmklxXc6cOV/ucfrR4+Wa6vO+LdT/YMSRFzynXLO4WtvHNovHyCRZu+NouWZhqTaveV6HFg+slWsuHX+yNP7snz5R7nEt1wxwrbV/nWu/Vfo9N3Q2AABc1/XOwD18xeX/Kckbd3EuAADswDUDXO/9rU9dbq39oyuvAwCwPypvrPvUKQDATcBXaQEADOZ6H2I4kz8783agtXb6qZty+W/DHd7NyQEA8Bdd73fg6n+vAQCAXeUtVACAwQhwAACDEeAAAAYjwAEADEaAAwAYjAAHADAYAQ4AYDACHADAYAQ4AIDBCHAAAIMR4AAABiPAAQAMRoADABiMAAcAMBgBDgBgMAIcAMBgBDgAgMEIcAAAgxHgAAAGI8ABAAxGgAMAGMzifk9gry0dWs/zv/6BHY9fWDtQa7CyWpxRkq3Nek21T2vlFuvzrMs8ei8N37zt+eUWh88cL9fk7Ona+IOH6z02N+o121u18avFfThJX5jUayZLtYLJHD3mmFebbpfGHzg1x74yz+O4tLzrPfq5M+WadvT2Wo/VtXKPm9X6VvG5leTY+fo2zrR2zJvrNaJP6zXFx36u/X6xeJxIyq9f07VDu94jSQ4Ujy0H5zm2JMlb3nnVxc7AAQAMRoADABiMAAcAMBgBDgBgMAIcAMBgBDgAgMEIcAAAgxHgAAAGI8ABAAxGgAMAGIwABwAwGAEOAGAwAhwAwGAEOACAwQhwAACDEeAAAAYjwAEADEaAAwAYjAAHADAYAQ4AYDACHADAYAQ4AIDBCHAAAIMR4AAABiPAAQAMRoADABiMAAcAMBgBDgBgMIv7PYG9tn3hYk7+3v+74/FtoZZxL506V51SltZXyzXLB2s1i+tr5R6LB+o1C3PU9M2t4viPlHucOXGqXLN9abM0fvnIerlHn/Zyzda5C6Xx063tco/V5xwt1ywerK1/W2jlHuf/9HPlml5c/3nmNVlZLtdsXbhUGj/P4zhZrc+rtdr6T4vP32S+7XXp1NlyzdKBlXJN1XRrOkdN7bGs7itJcuFk/bVoYXFSGn/k3ueWe8zz2FctHT5Yrtm+cLFcU80HkzleH6/FGTgAgMEIcAAAgxHgAAAGI8ABAAxGgAMAGIwABwAwGAEOAGAwAhwAwGAEOACAwQhwAACDEeAAAAYjwAEADGbfAlxrbdJa+0Br7Zdm1+9rrf1ma+2R1trPt9aWZ8tXZtcfmd1+7xX38X2z5R9vrX3j/qwJAMDe2s8zcP8wye9fcf1Hkryp9/5FSU4med1s+euSnJwtf9NsXFprL0ry2iRfmuQVSX6itTbZo7kDAOybfQlwrbW7k3xLkp+aXW9JviHJO2ZD3prkVbPLr5xdz+z2l83GvzLJ23rvl3rvf5TkkSQv2Zs1AADYP/t1Bu5fJfnvkkxn129P8mTvfWt2/dEkd80u35Xk00kyu/3UbPz/v/wqNQAAt6w9D3Cttf80yed677+zhz1f31p7uLX28PELl/aqLQDArljch55fm+RbW2vfnGQ1yeEkP5rkaGttcXaW7e4kj83GP5bkniSPttYWkxxJcvyK5U+5subP6b2/Ocmbk+Sv3Xlbv+FrBACwh/b8DFzv/ft673f33u/N5Q8hvLf3/l8keV+SV8+GPZjkF2eX3zm7ntnt7+2999ny184+pXpfkvuT/NYerQYAwL7ZjzNwz+S/T/K21toPJvlAkrfMlr8lyc+01h5JciKXQ1967x9trb09yceSbCV5Q+99e++nDQCwt/Y1wPXefzXJr84ufzJX+RRp7/1ikm9/hvofSvJDuzdDAICbj29iAAAYjAAHADAYAQ4AYDACHADAYAQ4AIDBCHAAAIO5mf4O3E1p9fbDpfHrX/Ccco+Vu+8u12RtrTZ+OscXUKys1Gu25/hTfBfO1cYv1ee1fFf9K9S2nniiVtDq/x9avGuOx766jTfn+Pq4hUm9Zu1AbfzWZrnF4efP8XXH1T59ev0xT7d+qF4zLfbZ3Kj3mOf5ePT20vDpSvFYlGThyeJzK8nBOdZleuZUrWBr6/pjnqZXH8ckbVJ7fvU51r3aI0kWDtVe77K0VO6RSxfrNYvFPhv158rWqSfLNQvF1+GF5TleU691fzf03gAA2HUCHADAYAQ4AIDBCHAAAIMR4AAABiPAAQAMRoADABiMAAcAMBgBDgBgMAIcAMBgBDgAgMEIcAAAgxHgAAAGI8ABAAxGgAMAGIwABwAwGAEOAGAwAhwAwGAEOACAwQhwAACDEeAAAAYjwAEADEaAAwAYjAAHADAYAQ4AYDACHADAYAQ4AIDBCHAAAINZ3O8J7LULxy/kI//2Izsev373Sun+Lx7frE4py0frD8NkqZa9ty5ul3usHq2t+7yWDiyVxl84caHcY3tzWq458/HzpfGLh+uP4z1f97xyzeJqcXudrK1Hkmyc3SjXTLd7afzFJy+Vexy551C5ZuvSVmn8uc/V96+l9dpjkiRLq5PS+PPHL5Z7zPMcPnL3kdL4lUOr5R6Pf+jxcs3FJ+r7y2StdpzcvlA/Tqwcqz/2q0dr22zrYm0fTpLDX3C4XLN27EBp/JnPnC73eOLjJ8o11celb9aORUl9X0mSQ3fXtlf1te56nIEDABiMAAcAMBgBDgBgMAIcAMBgBDgAgMEIcAAAgxHgAAAGI8ABAAxGgAMAGIwABwAwGAEOAGAwAhwAwGAEOACAwQhwAACDEeAAAAYjwAEADEaAAwAYjAAHADAYAQ4AYDACHADAYAQ4AIDBCHAAAIMR4AAABiPAAQAMRoADABiMAAcAMBgBDgBgMAIcAMBgFvd7Antt9dhKvuRv37/j8evPu710/yt33FadUrZOny3XbJ6/UBq/MJmUeywdPliuaQutXLN94VJp/LnPHC/3OHj3c8s1rbjNppub5R7zbOO+tV0a35aWyj2mGxvlmu1ztX3ywhNPlnusHFkv1ywU13+ytlLusXj0SLkmxf1r83Ofr/fYA33ayzW3v/gLyzXbl+r75GRludajeCxKks2z58o1VWvPu6NetFA/P9O3a8eWI/fVnvNJ8sK/tVaumazVaqrrkSSZ47Vr69SZ0viNJ0+XeyRJ3v0bV13sDBwAwGAEOACAwQhwAACDEeAAAAYjwAEADEaAAwAYjAAHADAYAQ4AYDACHADAYAQ4AIDBCHAAAIMR4AAABiPAAQAMRoADABiMAAcAMBgBDgBgMAIcAMBgBDgAgMEIcAAAgxHgAAAGI8ABAAxGgAMAGIwABwAwGAEOAGAwex7gWmv3tNbe11r7WGvto621fzhbfltr7d2ttU/M/j02W95aaz/WWnuktfah1tpXXnFfD87Gf6K19uBerwsAwH7YjzNwW0n+m977i5K8NMkbWmsvSvK9Sd7Te78/yXtm15Pkm5LcP/t5fZKfTC4HviRvTPLVSV6S5I1PhT4AgFvZnge43vvjvfffnV0+k+T3k9yV5JVJ3job9tYkr5pdfmWSn+6XvT/J0dba85N8Y5J3995P9N5PJnl3klfs4aoAAOyLxf1s3lq7N8lXJPnNJHf23h+f3fSZJHfOLt+V5NNXlD06W/ZMy69p6+JWnvjE53c8xyf/5MSOxybJ4kp9k146u1GuuXDiQmn86tHVco+j9xwt12xe2CzXXDpzsTS+T3u5x+lHj5drTnzyZK3HJ86Xe6w+d7lcs3ykto8dvutwucfWpa1yzXRru1xTtXGuPq+l1Ulp/B1fcuf1Bz3NPPvkwmLt/88XTtb3r3m0hVYaP8+8Vg7Vj0fL6/XnSnU/3rpYP37Nc8w79ekzpfFnH6kd75NkslY/P3P7lx8pjW+T2r6SJMvrK+Wa6rFlnuNXWxjvIwH7NuPW2sEk/2eSf9R7P33lbb33nqR+RHzmXq9vrYJdDQcAAAvnSURBVD3cWnv45Gb9yQYAcDPZlwDXWlvK5fD2s733X5gt/uzsrdHM/v3cbPljSe65ovzu2bJnWv4X9N7f3Ht/oPf+wLGlpRu3IgAA+2A/PoXakrwlye/33v/lFTe9M8lTnyR9MMkvXrH8O2efRn1pklOzt1rfleTlrbVjsw8vvHy2DADglrYfvwP3tUn+TpIPt9Y+OFv2PyT54SRvb629LsmnkrxmdtsvJ/nmJI8kOZ/ku5Ok936itfYDSX57Nu77e++1X1gDABjQnge43vtvJHmm33x82VXG9yRveIb7eijJQzdudgAAN7/xPnYBAPAsJ8ABAAxGgAMAGIwABwAwGAEOAGAwAhwAwGAEOACAwQhwAACDEeAAAAYjwAEADEaAAwAYjAAHADAYAQ4AYDACHADAYAQ4AIDBCHAAAIMR4AAABiPAAQAMRoADABiMAAcAMBgBDgBgMAIcAMBgBDgAgMEs7vcE9tryC+7OfT/2Izsev7G0Xrr/nladUjYnK+WavdDSyzUHtjfKNctb50vjpwv13XZrYblcs764Vhq/2eo9ppmUa6qPyyRb5R7zmPRan9an5R7z7JOXFmqP48Ycj8lS6vv9ZrHPPAfrSbbrNdPN0vjbNs+Ue2wvLJVr5jlOrk1r++TS9sVyj/T6PnlXq71OLBTXI0mmrb4fb09qx7B5evTiuifJ6kZtH1vYru3DSXJx5Ui5Zqu4vc4vHCr3SJJ80QuuutgZOACAwQhwAACDEeAAAAYjwAEADEaAAwAYjAAHADAYAQ4AYDACHADAYAQ4AIDBCHAAAIMR4AAABiPAAQAMRoADABiMAAcAMBgBDgBgMAIcAMBgBDgAgMEIcAAAgxHgAAAGI8ABAAxGgAMAGIwABwAwGAEOAGAwAhwAwGAEOACAwQhwAACDEeAAAAazuN8T2HOtZXuyvOPh62c/W7r7hY3z1RllY/32cs3W0lpp/MJ0q9xj+cKT5ZppYds+ZXN5vTR+a6HeY6Fvl2t6Wmn8ZurzWm6XyjUHL50sjZ9sb5R79IVJuabaZ2uyWu4xnWNea5dOlcavXKyNT5LJhTPlmu2VA7Xxxed8krQ+Ldf0Vvt//TzPrbZV3ycnmxfLNZtrR8o1VW2e9e+9VjDH47i5erhcM89jWbU4x2tkdRv3Vj9OHDz7mXLNdKEWoQ4Vx1+PM3AAAIMR4AAABiPAAQAMRoADABiMAAcAMBgBDgBgMAIcAMBgBDgAgMEIcAAAgxHgAAAGI8ABAAxGgAMAGIwABwAwGAEOAGAwAhwAwGAEOACAwQhwAACDEeAAAAYjwAEADEaAAwAYjAAHADAYAQ4AYDACHADAYAQ4AIDBCHAAAIMR4AAABiPAAQAMRoADABjM4n5PYK8dv7ien33kr+94fGu1+z93flqcUbKyUs/RJ05slMZfvLhV7nH29KVyzcXztXklydJKbTdcW18u91gp9kiSY8dWSuNPPVlf9xPHz5Vr1g/eURp/aY7Hfh4rq7Vt/PifHC/3OH/6fLlmul2b1/b2WrlHn95Zrllere1fy2v1/f7cqbPlmu2N2v5y8Wx9H07q65IcLFccufP20vijdxwr93jy8yfLNdOt2uvEZKl+/FpaWSrXrB2s7fut+gKZZHFpUq7Z3q5tr3On5tkn686ePF0av7hUf0yuxRk4AIDBCHAAAIMR4AAABiPAAQAMRoADABiMAAcAMBgBDgBgMAIcAMBgBDgAgMEMH+Baa69orX28tfZIa+1793s+AAC7begA11qbJPnxJN+U5EVJvqO19qL9nRUAwO4aOsAleUmSR3rvn+y9byR5W5JX7vOcAAB21egB7q4kn77i+qOzZQAAt6zF/Z7AXmitvT7J62dXL/3jVy58ZD/ncwt4TpIn9nsStwDb8cawHW8M2/HGsB1vDNvxz7zwagtHD3CPJbnniut3z5b9Ob33Nyd5c5K01h7uvT+wN9O7NdmGN4bteGPYjjeG7Xhj2I43hu14faO/hfrbSe5vrd3XWltO8tok79znOQEA7Kqhz8D13rdaa38/ybuSTJI81Hv/6D5PCwBgVw0d4JKk9/7LSX65UPLm3ZrLs4hteGPYjjeG7Xhj2I43hu14Y9iO19F67/s9BwAACkb/HTgAgGedZ02A85VbO3O97dRa+7rW2u+21rZaa69+2m3brbUPzn58mGRmB9v077XWPjzbbr/h20T+zE6ft621/6y11ltrD8yu39tau3DF/vi/7t2sb2472aattde01j7WWvtoa+3f7fUcb1Y7eC6/6Yp97g9aa09ecZvj41XsYJu+sLX2ntbah1prv9pau3s/5nlT6r3f8j+5/AGHP0zyhUmWk/xekhft97xutp+dbKck9yb58iQ/neTVT7vt7H6vw832s8NteviKy9+a5Ff2e943w89On7dJDiX59STvT/LAbNm9ST6y3+tws/3scH+8P8kHkhybXX/ufs/7Zvipvo4k+Qe5/MG6p647Ps6xTZP8H0kenF3+hiQ/s9/zvll+ni1n4Hzl1s5cdzv13v+49/6hJNP9mOCAdrJNT19xdT2JX0y9bKfP2x9I8iNJLu7l5Aa1k236d5P8eO/9ZJL03j+3x3O8WVVfR74jyc/tyczGtZNt+qIk751dft9Vbn/WerYEOF+5tTN/2e202lp7uLX2/tbaq27s1Ia1o23aWntDa+0Pk/zzJN+zR3O72V1327XWvjLJPb33/3CV+vtaax9orf1aa+1v7uI8R7KT/fGLk3xxa+3/nj2XX7Fns7u57fj42Fp7YZL78mfBI3F8vJqdbNPfS/Jts8t/O8mh1trtezC3m97wf0aEm8oLe++Ptda+MMl7W2sf7r3/4X5PagS99x9P8uOttf88yT9N8uA+T+mm11pbSPIvk3zXVW5+PMkLeu/HW2tfleTft9a+9GlnO7m6xVx+G/Xrc/nbbX69tfZlvfcnr1nFlV6b5B299+0rljk+zue/TfK/tNa+K5d/VeKxJNvXrHiWeLacgdvRV27xl9tOvffHZv9+MsmvJvmKGzm5QVW36duS+N/5ZdfbdoeSvDjJr7bW/jjJS5O8s7X2QO/9Uu/9eJL03n8nl3/P5ov3ZNY3t53sj48meWfvfbP3/kdJ/iCXA92zXeW5/No87e1Tx8eruu427b3/ae/923rvX5Hkf5wt85+JPHsCnK/c2pm5t1Nr7VhrbWV2+TlJvjbJx3ZtpuO47jZtrV354vgtST6xh/O7mV1z2/XeT/Xen9N7v7f3fm8uf4jhW3vvD7fW7mitTZJkdsbj/iSf3PtVuOns5Dn+73P57NtTz+Uvjm2X7PD42Fr7kiTHkvw/VyxzfLy6nRwfnzM7254k35fkoT2e403rWRHgeu9bSZ76yq3fT/L27iu3/oJn2k6tte9vrX1rkrTW/npr7dEk357kf2utPbUd/5MkD7fWfi+Xf9H0h3vvz/oD1E62aZK/P/tzDR9M8l/H26dJdrztnsnXJfnQbJu+I8nf672f2N0Z3/x2uE3fleR4a+1jufxc/idPnc18Nivsj69N8rbe+5UfRnJ8vIodbtOvT/Lx1tofJLkzyQ/ty2RvQr6JAQBgMM+KM3AAALcSAQ4AYDACHADAYAQ4AIDBCHAAAIMR4ACeprV2e2vtg7Ofz7TWHptdPtta+4n9nh+APyMCcA2ttX+W5Gzv/V/s91wAnuIMHMAOtda+vrX2S7PL/6y19tbW2v/VWvtUa+3bWmv/vLX24dbar7TWlmbjvqq19muttd9prb2rtfb8/V0L4FYgwAHM768k+YYk35rk3yZ5X+/9y5JcSPItsxD3r5O8uvf+Vbn8NUD+kjzwl7a43xMAGNh/7L1vttY+nGSS5Fdmyz+c5N4kfzXJi5O8u7WW2ZjH92GewC1GgAOY36Uk6b1PW2ubV3z/5TSXj68tyUd7739jvyYI3Jq8hQqwez6e5I7W2t9IktbaUmvtS/d5TsAtQIAD2CW9940kr07yI62130vywSRfs7+zAm4F/owIAMBgnIEDABiMAAcAMBgBDgBgMAIcAMBgBDgAgMEIcAAAgxHgAAAGI8ABAAzm/wMWY+2s4dQdQwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_mfcc=20\n",
    "sample_duration=1\n",
    "\n",
    "sr=default_sr\n",
    "sample_size=sample_duration*sr\n",
    "sample_size_cnn=sample_size\n",
    "\n",
    "testsample,_,_=sample_generator(all_clips['cold'],sample_duration,random=True).__next__()\n",
    "#mfcc=librosa.feature.melspectrogram(testsample, sr=sr, n_mfcc=n_mfcc)\n",
    "test_data=librosa.feature.mfcc(testsample, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "(width,height)=test_data.shape\n",
    "plt.figure(figsize=(10,10))\n",
    "librosa.display.specshow(test_data, x_axis='time', y_axis='linear')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting train data...\n",
      "getting test data...\n",
      "loaded data\n"
     ]
    }
   ],
   "source": [
    "# Also hyperparameters!\n",
    "n_mfcc=20\n",
    "sample_duration=1\n",
    "\n",
    "# generate training samples\n",
    "print(\"getting train data...\")\n",
    "train_clips=[]\n",
    "for label in labels:\n",
    "    train_clips+=[clip for clip in all_clips[label] if (clip.sr==sr and clip.kind=='train' and clip.size>=sample_size)]\n",
    "    \n",
    "n_samples=sum([1 for _,_,_, in sample_generator(train_clips,sample_duration)])\n",
    "n_samples=1000\n",
    "X=np.zeros((n_samples,width,height,1))\n",
    "y=np.zeros((n_samples,num_labels))\n",
    "\n",
    "def getdata_cnn(sample,params):\n",
    "    n_mfcc=params\n",
    "    return np.expand_dims(librosa.feature.mfcc(sample, sr=sr, n_mfcc=n_mfcc),axis=2)\n",
    "\n",
    "params_cnn=n_mfcc\n",
    "\n",
    "ks=lambda c : 'e' in c.name\n",
    "\n",
    "for i,(sample,_,label) in enumerate(sample_generator(train_clips,sample_duration,random=True,max_iters=n_samples,clip_filter=ks)):\n",
    "    X[i,:,:,:]=getdata_cnn(sample,params_cnn)\n",
    "    y[i,label2ind[label]]=1\n",
    "\n",
    "# generate testing samples \n",
    "print(\"getting test data...\")\n",
    "test_clips=[]\n",
    "for label in labels:\n",
    "    test_clips+=[clip for clip in all_clips[label] if (clip.sr==sr and clip.kind=='test' and clip.size>=sample_size)]\n",
    "    \n",
    "\n",
    "n_tests=sum([1 for _,_,_, in sample_generator(test_clips,sample_duration)])\n",
    "n_tests=200\n",
    "X_test=np.zeros((n_tests,width,height,1))\n",
    "y_test=np.zeros((n_tests,num_labels))\n",
    "\n",
    "for i,(sample,_,label) in enumerate(sample_generator(test_clips,sample_duration,random=True,max_iters=n_tests,clip_filter=ks)):\n",
    "    X_test[i,:,:,:]=getdata_cnn(sample,params_cnn)\n",
    "    y_test[i,label2ind[label]]=1\n",
    "\n",
    "hyperparam_acc={}\n",
    "print(\"loaded data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 1.5573 - accuracy: 0.9131 - val_loss: 0.1177 - val_accuracy: 0.9740\n",
      "Epoch 2/5\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1155 - accuracy: 0.9665 - val_loss: 0.1072 - val_accuracy: 0.9715\n",
      "Epoch 3/5\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0768 - accuracy: 0.9772 - val_loss: 0.0752 - val_accuracy: 0.9740\n",
      "Epoch 4/5\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0467 - accuracy: 0.9850 - val_loss: 0.0599 - val_accuracy: 0.9850\n",
      "Epoch 5/5\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.0412 - accuracy: 0.9870 - val_loss: 0.0792 - val_accuracy: 0.9805\n",
      "1/1 [==============================] - 0s 896us/step - loss: 0.2173 - accuracy: 0.9055\n",
      "best so far!\n"
     ]
    }
   ],
   "source": [
    "# simple model \n",
    "\n",
    "# test hyperparameters\n",
    "batch_size = 16\n",
    "conv1=(16,(3,3),1)  # (num_filters, (filtersize), strides)\n",
    "conv2=(32,(3,3),1) \n",
    "dense=64\n",
    "hyperparams=(batch_size,conv1,dense)\n",
    "\n",
    "clf_cnns = Sequential()\n",
    "clf_cnns.add(Conv2D(conv1[0], kernel_size=conv1[1],\n",
    "                strides=conv1[2],input_shape=(width,height,1)))\n",
    "clf_cnns.add(Conv2D(conv2[0], kernel_size=conv2[1],\n",
    "                strides=conv2[2],input_shape=(width,height,1)))\n",
    "clf_cnns.add(Flatten())\n",
    "clf_cnns.add(Dense(dense, activation='relu'))\n",
    "clf_cnns.add(Dense(num_labels, activation='softmax'))\n",
    "\n",
    "clf_cnns.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "clf_cnns.fit(X, y,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5,\n",
    "          validation_split = 0.2)\n",
    "\n",
    "results = clf_cnns.evaluate(X_test, y_test, batch_size=n_tests)\n",
    "\n",
    "hyperparam_acc[hyperparams]=results[1]\n",
    "if results[1]==max(hyperparam_acc.values()):\n",
    "    print(\"best so far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cold', 'cold', 'hot', 'cold', 'cold', 'cold', 'cold', 'cold', 'hot', 'cold']\n"
     ]
    }
   ],
   "source": [
    "cnns_predictor=predictor(clf_cnns,getdata_cnn,params_cnn,sample_size_cnn,ohe=True)\n",
    "\n",
    "# pick 10 random 'cold' samples to test\n",
    "to_test=[sample for sample,_,_ in sample_generator(all_clips['cold'], sample_size_log_spec/default_sr, max_iters=10,random=True,clip_filter=ks)]\n",
    "\n",
    "print(cnns_predictor.test_samples(to_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "water is currently: <font color='red'>HOT</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnns_predictor.continuous_record_and_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "250/250 [==============================] - 6s 24ms/step - loss: 0.4028 - accuracy: 0.8940 - val_loss: 0.3590 - val_accuracy: 0.9605\n",
      "Epoch 2/10\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 0.2334 - accuracy: 0.9826 - val_loss: 0.1968 - val_accuracy: 0.9810\n",
      "Epoch 3/10\n",
      "250/250 [==============================] - 6s 22ms/step - loss: 0.1703 - accuracy: 0.9872 - val_loss: 0.1434 - val_accuracy: 0.9890\n",
      "Epoch 4/10\n",
      "250/250 [==============================] - 6s 22ms/step - loss: 0.1321 - accuracy: 0.9891 - val_loss: 0.1080 - val_accuracy: 0.9935\n",
      "Epoch 5/10\n",
      "250/250 [==============================] - 6s 22ms/step - loss: 0.0995 - accuracy: 0.9923 - val_loss: 0.1025 - val_accuracy: 0.9835\n",
      "Epoch 6/10\n",
      "250/250 [==============================] - 6s 22ms/step - loss: 0.0748 - accuracy: 0.9959 - val_loss: 0.0648 - val_accuracy: 0.9970\n",
      "Epoch 7/10\n",
      "250/250 [==============================] - 6s 22ms/step - loss: 0.0605 - accuracy: 0.9960 - val_loss: 0.1340 - val_accuracy: 0.9590\n",
      "Epoch 8/10\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 0.0472 - accuracy: 0.9980 - val_loss: 0.0417 - val_accuracy: 0.9980\n",
      "Epoch 9/10\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 0.0403 - accuracy: 0.9977 - val_loss: 0.0438 - val_accuracy: 0.9945\n",
      "Epoch 10/10\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 0.0602 - accuracy: 0.9890 - val_loss: 0.0306 - val_accuracy: 0.9985\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4272 - accuracy: 0.8410\n",
      "best so far!\n"
     ]
    }
   ],
   "source": [
    "# model inspired by YAMnet (https://github.com/tensorflow/models/tree/master/research/audioset/yamnet)\n",
    "\n",
    "# test hyperparameters\n",
    "batch_size = 32\n",
    "conv1=(32,(3,3),2)  # (num_filters, (filtersize), strides)\n",
    "conv2=(1,(3,3),1)\n",
    "conv3=(64,(3,3),1)\n",
    "dense=16\n",
    "hyperparams=(batch_size,conv1,conv2,conv3,dense)\n",
    "\n",
    "clf_cnn = Sequential()\n",
    "clf_cnn.add(Conv2D(conv1[0], kernel_size=conv1[1],\n",
    "                strides=conv1[2],input_shape=(width,height,1)))\n",
    "clf_cnn.add(BatchNormalization(center = True,scale= False, epsilon= 1e-4))\n",
    "clf_cnn.add(ReLU())\n",
    "clf_cnn.add(DepthwiseConv2D(depth_multiplier=conv2[0], kernel_size=conv2[1],\n",
    "                strides=conv2[2], padding='same',use_bias=False,activation=None))\n",
    "clf_cnn.add(BatchNormalization(center = True,scale= False, epsilon= 1e-4))\n",
    "clf_cnn.add(ReLU())\n",
    "clf_cnn.add(Conv2D(conv3[0], kernel_size=conv3[1],\n",
    "                strides=conv3[2],input_shape=(width,height,1)))\n",
    "clf_cnn.add(BatchNormalization(center = True,scale= False, epsilon= 1e-4))\n",
    "clf_cnn.add(ReLU())\n",
    "clf_cnn.add(Flatten())\n",
    "clf_cnn.add(Dense(dense, activation='relu'))\n",
    "clf_cnn.add(Dense(num_labels, activation='softmax'))\n",
    "\n",
    "clf_cnn.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "clf_cnn.fit(X, y,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          validation_split = 0.2)\n",
    "\n",
    "results = clf_cnn.evaluate(X_test, y_test, batch_size=n_tests)\n",
    "\n",
    "hyperparam_acc[hyperparams]=results[1]\n",
    "if results[1]==max(hyperparam_acc.values()):\n",
    "    print(\"best so far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hot', 'hot', 'hot', 'hot', 'hot', 'hot', 'hot', 'hot', 'hot', 'hot']\n"
     ]
    }
   ],
   "source": [
    "cnn_predictor=predictor(clf_cnn,getdata_cnn,params_cnn,sample_size_cnn,ohe=True)\n",
    "\n",
    "# pick 10 random 'cold' samples to test\n",
    "to_test=[sample for sample,_,_ in sample_generator(all_clips['hot'], sample_size_log_spec/default_sr, max_iters=10,random=True,clip_filter=ks)]\n",
    "\n",
    "print(cnn_predictor.test_samples(to_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "water is currently: <font color='blue'>COLD</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnn_predictor.continuous_record_and_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "    - transfer learning from YAMnet or other audio classification network\n",
    "    - try with more training data (quantity and variety)\n",
    "    - clean audio data somehow\n",
    "    - get continuous temperature rather than 'hot' vs 'cold'\n",
    "\n",
    "APPLICATIONS:\n",
    "    - just for fun!\n",
    "    - google home-type device to estimate temperature of tap water, give warning when very hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
